{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from retain_torch.Retain_torch import Retain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adadelta\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from utils import metric_report\n",
    "import json\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Neccesery Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_mask2(out, data_len, device=torch.device('cuda:0')):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    :param out: network output\n",
    "    :param data_len: length list\n",
    "    :param device: gpu\n",
    "    :return: mask matrix\n",
    "    \"\"\"\n",
    "    mask = torch.zeros_like(out, device=device)\n",
    "    # Determine 123 or 213\n",
    "    if mask.size(0) == len(data_len):\n",
    "        for i, l in enumerate(data_len):\n",
    "            # 对应位置长度-1\n",
    "            mask[i, :l-1] = 1\n",
    "    else:\n",
    "        for i, l in enumerate(data_len):\n",
    "            # 对应位置长度-1\n",
    "            mask[:l-1, i] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_squeeze(gbert_pred, gbert_true, gbert_len):\n",
    "    \"\"\"\n",
    "    Flatten the network output of multi-label classification and the ground truth into a binary classification format\n",
    "\n",
    "    :param gbert_pred: network output: shape=(seq_len, batch_size, out_dim)\n",
    "    :param gbert_true: true label: shape=(seq_len, batch_size, out_dim)\n",
    "    :param gbert_len: sequence length: list\n",
    "    :return: ndarray, ndarray, shape=(seq_len*batch_size*out_dim, 1)\n",
    "    \"\"\"\n",
    "    y_pred_all = []\n",
    "    y_true_all = []\n",
    "    for b in range(len(gbert_len)):\n",
    "        y_pred = np.transpose(gbert_pred[b], (1, 0, 2))\n",
    "        y_true = np.transpose(gbert_true[b], (1, 0, 2))\n",
    "        v_len = gbert_len[b]\n",
    "        for p in range(y_pred.shape[0]):\n",
    "            for v in range(v_len[p] - 1):\n",
    "                y_pred_all.append(y_pred[p][v].reshape(1, -1))  # shape:(283, )->(1, 283)\n",
    "                y_true_all.append(y_true[p][v].reshape(1, -1))  # shape:(283, )->(1, 283)\n",
    "    y_pred_all = np.concatenate(y_pred_all)     # shape=(seq_len*batch_size*out_dim, 1)\n",
    "    y_true_all = np.concatenate(y_true_all)     # shape=(seq_len*batch_size*out_dim, 1)\n",
    "    return y_pred_all, y_true_all\n",
    "\n",
    "\n",
    "# Calculate the topk according to the training y_hat and y\n",
    "# Here y_true is the actual input without padTrainMatrix\n",
    "def validation(y_hat, y_true, length, topk):\n",
    "    # Change the dimension to maxlen × n_samples × outputDimSize\n",
    "    y_hat = np.transpose(y_hat, (1, 0, 2))\n",
    "\n",
    "    NDCG = 0.0\n",
    "    RECALL = 0.0\n",
    "    daynum = 0\n",
    "\n",
    "    n_patients = y_hat.shape[0]\n",
    "    for i in range(n_patients):\n",
    "        predict_one = y_hat[i]\n",
    "        y_true_one = y_true[i]\n",
    "        len_one = length[i]\n",
    "\n",
    "        # Subtract 1 because the prediction is for the 2nd~nth day, excluding the first day\n",
    "        for i in range(len_one - 1):\n",
    "            y_pred_day = predict_one[i]\n",
    "            y_true_day = y_true_one[i + 1]\n",
    "            ndcg, lyt, ltp = evaluate_predict_performance(y_pred_day.flatten(), y_true_day, topk)\n",
    "            NDCG += ndcg\n",
    "            recall = 0.0\n",
    "            if lyt != 0:\n",
    "                recall += ltp * 1.0 / lyt\n",
    "            else:\n",
    "                recall += 1.0\n",
    "            RECALL += recall\n",
    "            daynum += 1\n",
    "\n",
    "    return NDCG, RECALL, daynum\n",
    "\n",
    "\n",
    "# Calculate topk for each day\n",
    "def evaluate_predict_performance(y_pred, y_bow_true, topk=30):\n",
    "    sorted_idx_y_pred = np.argsort(-y_pred)\n",
    "\n",
    "    if topk == 0:\n",
    "        sorted_idx_y_pred_topk = sorted_idx_y_pred[:len(y_bow_true)]\n",
    "    else:\n",
    "        sorted_idx_y_pred_topk = sorted_idx_y_pred[:topk]\n",
    "\n",
    "    sorted_idx_y_true = y_bow_true\n",
    "\n",
    "    true_part = set(sorted_idx_y_true).intersection(set(sorted_idx_y_pred_topk))  # Coincidence part, used to calculate ndcg\n",
    "    idealDCG = 0.0\n",
    "    for i in range(len(sorted_idx_y_true)):\n",
    "        idealDCG += (2 ** 1 - 1) / math.log(1 + i + 1)\n",
    "\n",
    "    DCG = 0.0\n",
    "    for i in range(len(sorted_idx_y_true)):\n",
    "        if sorted_idx_y_true[i] in true_part:\n",
    "            DCG += (2 ** 1 - 1) / math.log(1 + i + 1)\n",
    "\n",
    "    # print('true lab size: %d, intersection part size: %d' %(len(sorted_idx_y_true), len(true_part)))\n",
    "    if idealDCG != 0:\n",
    "        NDCG = DCG / idealDCG\n",
    "    else:\n",
    "        NDCG = 1\n",
    "    # print('NDCG: ' + str(NDCG))\n",
    "    return NDCG, len(sorted_idx_y_true), len(true_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jongwook95.lee/study/dignosis_analysis/retain/parameters.params\", 'r') as cfg:\n",
    "    config = json.load(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeddxs_dir = config['parameters']['encodeddxs_dir']\n",
    "train_size = config['parameters']['train_size']\n",
    "epochs = config['parameters']['epochs']\n",
    "batch_size = config['parameters']['batch_size']\n",
    "learning_rate = config['parameters']['learning_rate']\n",
    "diagnosis_size = config['parameters']['diagnosis_size']\n",
    "L2_decay = config['parameters']['L2_decay']\n",
    "best_metric = config['parameters']['best_metric']\n",
    "topk = config['parameters']['topk']\n",
    "save_best_model = config['parameters']['save_best_model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_best_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['parameters']['encodeddxs_dir'], 'rb') as f:\n",
    "    patients = pickle.load(f)\n",
    "\n",
    "patients_num = len(patients)\n",
    "train_patient_num = int(patients_num * config['parameters']['train_size'])\n",
    "patients_train = patients[0:train_patient_num]\n",
    "test_patient_num = patients_num - train_patient_num\n",
    "patients_test = patients[train_patient_num:]\n",
    "\n",
    "train_batch_num = int(np.ceil(float(train_patient_num) / batch_size))\n",
    "test_batch_num = int(np.ceil(float(test_patient_num) / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of the layer: [300, 512]\n",
      "Layer parameters: 153600\n",
      "Structure of the layer: [300]\n",
      "Layer parameters: 300\n",
      "Structure of the layer: [600, 300]\n",
      "Layer parameters: 180000\n",
      "Structure of the layer: [600, 200]\n",
      "Layer parameters: 120000\n",
      "Structure of the layer: [600]\n",
      "Layer parameters: 600\n",
      "Structure of the layer: [600]\n",
      "Layer parameters: 600\n",
      "Structure of the layer: [600, 300]\n",
      "Layer parameters: 180000\n",
      "Structure of the layer: [600, 200]\n",
      "Layer parameters: 120000\n",
      "Structure of the layer: [600]\n",
      "Layer parameters: 600\n",
      "Structure of the layer: [600]\n",
      "Layer parameters: 600\n",
      "Structure of the layer: [1, 200]\n",
      "Layer parameters: 200\n",
      "Structure of the layer: [1]\n",
      "Layer parameters: 1\n",
      "Structure of the layer: [300, 200]\n",
      "Layer parameters: 60000\n",
      "Structure of the layer: [300]\n",
      "Layer parameters: 300\n",
      "Structure of the layer: [512, 300]\n",
      "Layer parameters: 153600\n",
      "Structure of the layer: [512]\n",
      "Layer parameters: 512\n",
      "The total number of parameters: 970913\n"
     ]
    }
   ],
   "source": [
    "model = Retain(inputDimSize=diagnosis_size, embDimSize=300, alphaHiddenDimSize=200, betaHiddenDimSize=200, outputDimSize=diagnosis_size)\n",
    "\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    print(\"Structure of the layer: \" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    print(\"Layer parameters: \" + str(l))\n",
    "    k = k + l\n",
    "\n",
    "print(\"The total number of parameters: {}\".format(k))\n",
    "model = model.cuda(device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adadelta(model.parameters(), lr=learning_rate, weight_decay=L2_decay)\n",
    "loss_mce = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-0:225599.32759094238 Train Time:2.7468905448913574\n",
      "Test:Epoch-0 Loss:15113.606658935547 Test Time:3.73496150970459\n",
      "Test:Epoch-0 NDCG:0.5580812336111073 RECALL:0.5339727971865\n",
      "Test:Epoch-0 Jaccard:0.10414779078049213 f1:0.16716299340304747 prauc:0.21867805018680594 roauc:0.859035229384661\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-1:187567.06274414062 Train Time:2.6364502906799316\n",
      "Test:Epoch-1 Loss:14217.854721069336 Test Time:3.6145713329315186\n",
      "Test:Epoch-1 NDCG:0.5585186787383105 RECALL:0.5347717444781064\n",
      "Test:Epoch-1 Jaccard:0.09828365070168335 f1:0.15466423026083229 prauc:0.2198654823424426 roauc:0.858555606179772\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-2:169788.928024292 Train Time:2.690964698791504\n",
      "Test:Epoch-2 Loss:13911.332977294922 Test Time:3.6253857612609863\n",
      "Test:Epoch-2 NDCG:0.5583896104216386 RECALL:0.534748439887102\n",
      "Test:Epoch-2 Jaccard:0.1057373688442572 f1:0.16529872647844895 prauc:0.22093842569950603 roauc:0.8585050875071354\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-3:165604.56370544434 Train Time:2.641319513320923\n",
      "Test:Epoch-3 Loss:13798.739959716797 Test Time:3.625678539276123\n",
      "Test:Epoch-3 NDCG:0.5643503086394001 RECALL:0.5411792123048238\n",
      "Test:Epoch-3 Jaccard:0.10520368308069898 f1:0.16429196968932858 prauc:0.22475331354131864 roauc:0.8587247040046879\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-4:163336.2516860962 Train Time:2.6547887325286865\n",
      "Test:Epoch-4 Loss:13898.55502319336 Test Time:3.6404001712799072\n",
      "Test:Epoch-4 NDCG:0.5667585682789538 RECALL:0.5433996219477458\n",
      "Test:Epoch-4 Jaccard:0.1137067496163249 f1:0.17747234855366892 prauc:0.22883945931715033 roauc:0.8591324870893013\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-5:161132.4055404663 Train Time:2.6460132598876953\n",
      "Test:Epoch-5 Loss:13628.597412109375 Test Time:3.6372108459472656\n",
      "Test:Epoch-5 NDCG:0.5700088490373215 RECALL:0.5452004312526333\n",
      "Test:Epoch-5 Jaccard:0.11110663491453862 f1:0.17201940515133576 prauc:0.23318797947709136 roauc:0.8599387271558684\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-6:160668.8103942871 Train Time:2.687798500061035\n",
      "Test:Epoch-6 Loss:13437.808738708496 Test Time:3.671530246734619\n",
      "Test:Epoch-6 NDCG:0.5772156775043857 RECALL:0.551725716733873\n",
      "Test:Epoch-6 Jaccard:0.10463029482496405 f1:0.16255276376981537 prauc:0.24357464631138784 roauc:0.8604912838882681\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-7:158859.1750793457 Train Time:2.6994855403900146\n",
      "Test:Epoch-7 Loss:13284.566131591797 Test Time:3.683309555053711\n",
      "Test:Epoch-7 NDCG:0.5842336520897321 RECALL:0.5588517427832271\n",
      "Test:Epoch-7 Jaccard:0.10247470714310193 f1:0.15870569280864283 prauc:0.2542327804911679 roauc:0.8618179959371913\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-8:157563.3221282959 Train Time:2.6391501426696777\n",
      "Test:Epoch-8 Loss:13280.215438842773 Test Time:3.626950979232788\n",
      "Test:Epoch-8 NDCG:0.592541660432819 RECALL:0.5674085294803074\n",
      "Test:Epoch-8 Jaccard:0.10307064775206028 f1:0.16007063826067414 prauc:0.27695085842372563 roauc:0.8632049141382102\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-9:156540.3448791504 Train Time:2.713371992111206\n",
      "Test:Epoch-9 Loss:13161.936210632324 Test Time:3.6966707706451416\n",
      "Test:Epoch-9 NDCG:0.6022606373054646 RECALL:0.5761372758110114\n",
      "Test:Epoch-9 Jaccard:0.11487420633615815 f1:0.17692297011661712 prauc:0.293318383084739 roauc:0.864073229539332\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-10:154749.6488647461 Train Time:2.6794700622558594\n",
      "Test:Epoch-10 Loss:13062.12370300293 Test Time:3.664903402328491\n",
      "Test:Epoch-10 NDCG:0.608521178961751 RECALL:0.5806764922588737\n",
      "Test:Epoch-10 Jaccard:0.134050459374009 f1:0.201504989185028 prauc:0.3234149821453972 roauc:0.8657673998297841\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-11:152915.62171936035 Train Time:2.6627469062805176\n",
      "Test:Epoch-11 Loss:12913.018692016602 Test Time:3.6578502655029297\n",
      "Test:Epoch-11 NDCG:0.6183783712920028 RECALL:0.5899452459811348\n",
      "Test:Epoch-11 Jaccard:0.14461587003449589 f1:0.2161109128207447 prauc:0.35188994848045024 roauc:0.8674717121815063\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-12:151853.48722839355 Train Time:2.575763463973999\n",
      "Test:Epoch-12 Loss:12947.064651489258 Test Time:3.555979013442993\n",
      "Test:Epoch-12 NDCG:0.623786846720327 RECALL:0.595082613598111\n",
      "Test:Epoch-12 Jaccard:0.16358694776313046 f1:0.24018655006084236 prauc:0.384228141372176 roauc:0.869090173267638\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-13:150390.47360229492 Train Time:2.6761345863342285\n",
      "Test:Epoch-13 Loss:12527.708503723145 Test Time:3.672863006591797\n",
      "Test:Epoch-13 NDCG:0.6309443528459112 RECALL:0.6014461793417746\n",
      "Test:Epoch-13 Jaccard:0.22496083567587385 f1:0.31146517558528386 prauc:0.4036905573832857 roauc:0.8706017911684377\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-14:148844.3005218506 Train Time:2.6431772708892822\n",
      "Test:Epoch-14 Loss:12636.690063476562 Test Time:3.6332671642303467\n",
      "Test:Epoch-14 NDCG:0.6312937703869989 RECALL:0.6021464940714529\n",
      "Test:Epoch-14 Jaccard:0.22185403991101238 f1:0.3047955870651627 prauc:0.4233948304649199 roauc:0.8721396327899015\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-15:148920.22999572754 Train Time:2.719414472579956\n",
      "Test:Epoch-15 Loss:12244.16586303711 Test Time:3.730795383453369\n",
      "Test:Epoch-15 NDCG:0.6374023244515599 RECALL:0.6087541695208905\n",
      "Test:Epoch-15 Jaccard:0.25838528880943235 f1:0.3514105355264906 prauc:0.4349130310975915 roauc:0.8734961555400822\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-16:147952.29092407227 Train Time:2.6606950759887695\n",
      "Test:Epoch-16 Loss:12161.226676940918 Test Time:3.6493053436279297\n",
      "Test:Epoch-16 NDCG:0.6402903844185304 RECALL:0.6119673782805919\n",
      "Test:Epoch-16 Jaccard:0.26978726366980854 f1:0.3646631640004026 prauc:0.4428702476392297 roauc:0.8745405719576105\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-17:147335.09155273438 Train Time:2.6619763374328613\n",
      "Test:Epoch-17 Loss:12080.18416595459 Test Time:3.644489049911499\n",
      "Test:Epoch-17 NDCG:0.6444877477746354 RECALL:0.6157776789098158\n",
      "Test:Epoch-17 Jaccard:0.2913738345749586 f1:0.3887058029786458 prauc:0.4541745851917493 roauc:0.8755631423255358\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-18:144861.6923751831 Train Time:2.6450977325439453\n",
      "Test:Epoch-18 Loss:11847.953273773193 Test Time:3.6312856674194336\n",
      "Test:Epoch-18 NDCG:0.645224576885119 RECALL:0.6177857578346974\n",
      "Test:Epoch-18 Jaccard:0.3024974748594565 f1:0.3994778944015303 prauc:0.46403675461991756 roauc:0.8764547295786149\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-19:147251.39617919922 Train Time:2.602937936782837\n",
      "Test:Epoch-19 Loss:11781.904277801514 Test Time:3.5898990631103516\n",
      "Test:Epoch-19 NDCG:0.6487791368044308 RECALL:0.6209370564182961\n",
      "Test:Epoch-19 Jaccard:0.3049899773162896 f1:0.4024098128486721 prauc:0.4721169673450646 roauc:0.8771533129511111\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-20:144911.42176818848 Train Time:2.603430986404419\n",
      "Test:Epoch-20 Loss:11679.310527801514 Test Time:3.6119439601898193\n",
      "Test:Epoch-20 NDCG:0.6542670887939182 RECALL:0.6256497625991914\n",
      "Test:Epoch-20 Jaccard:0.3151587423814178 f1:0.41293475088557385 prauc:0.48029031929738564 roauc:0.8777961461843803\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-21:144795.2636871338 Train Time:2.6224465370178223\n",
      "Test:Epoch-21 Loss:11607.657417297363 Test Time:3.6042673587799072\n",
      "Test:Epoch-21 NDCG:0.6622982658208705 RECALL:0.6337196245842247\n",
      "Test:Epoch-21 Jaccard:0.325142389783499 f1:0.4230452673812235 prauc:0.48933842067590483 roauc:0.8790163942601226\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-22:147530.56746673584 Train Time:2.6219210624694824\n",
      "Test:Epoch-22 Loss:11452.419342041016 Test Time:3.606651544570923\n",
      "Test:Epoch-22 NDCG:0.6625591312214381 RECALL:0.6349819565969645\n",
      "Test:Epoch-22 Jaccard:0.331401170879148 f1:0.4317663513299603 prauc:0.4941739218934681 roauc:0.879347421057995\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-23:143362.63277435303 Train Time:2.6468472480773926\n",
      "Test:Epoch-23 Loss:11377.168556213379 Test Time:3.6303253173828125\n",
      "Test:Epoch-23 NDCG:0.6692090300855682 RECALL:0.6430932489660056\n",
      "Test:Epoch-23 Jaccard:0.34360275848856603 f1:0.4426032071112349 prauc:0.5007758359134373 roauc:0.8800818527224421\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-24:141902.40954589844 Train Time:2.614497184753418\n",
      "Test:Epoch-24 Loss:11405.248264312744 Test Time:3.600590467453003\n",
      "Test:Epoch-24 NDCG:0.6692705905315502 RECALL:0.6432356659110328\n",
      "Test:Epoch-24 Jaccard:0.330066027863744 f1:0.4285620373399857 prauc:0.5035245691189033 roauc:0.8802055947794416\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-25:142998.42776489258 Train Time:2.635723352432251\n",
      "Test:Epoch-25 Loss:11375.520050048828 Test Time:3.6244149208068848\n",
      "Test:Epoch-25 NDCG:0.6709876889713129 RECALL:0.6455622409129749\n",
      "Test:Epoch-25 Jaccard:0.3355671743518399 f1:0.43605967467527373 prauc:0.5073127155925168 roauc:0.8806196116474794\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-26:141101.29152679443 Train Time:2.594226121902466\n",
      "Test:Epoch-26 Loss:11258.603485107422 Test Time:3.5766048431396484\n",
      "Test:Epoch-26 NDCG:0.6715389633299521 RECALL:0.6464322789771402\n",
      "Test:Epoch-26 Jaccard:0.3411175692170798 f1:0.44128892068755393 prauc:0.5103862562608434 roauc:0.881060802894751\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-27:142459.22147369385 Train Time:2.66733980178833\n",
      "Test:Epoch-27 Loss:11158.82725906372 Test Time:3.6470835208892822\n",
      "Test:Epoch-27 NDCG:0.671473998932598 RECALL:0.6465643383261652\n",
      "Test:Epoch-27 Jaccard:0.3486376592412481 f1:0.44858020439664786 prauc:0.5131242105653533 roauc:0.8810075728055619\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-28:141485.658203125 Train Time:2.6407816410064697\n",
      "Test:Epoch-28 Loss:11155.097316741943 Test Time:3.618943929672241\n",
      "Test:Epoch-28 NDCG:0.6779173153076311 RECALL:0.6539285890835644\n",
      "Test:Epoch-28 Jaccard:0.3563765625348007 f1:0.4561734654468709 prauc:0.5211992232184939 roauc:0.8817480147289963\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-29:140863.71096038818 Train Time:2.6443920135498047\n",
      "Test:Epoch-29 Loss:11121.496662139893 Test Time:3.625361204147339\n",
      "Test:Epoch-29 NDCG:0.6775289883830928 RECALL:0.6535110484947351\n",
      "Test:Epoch-29 Jaccard:0.36405696442401175 f1:0.4634637160648656 prauc:0.5223719162934516 roauc:0.8820487364023241\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-30:139348.25479125977 Train Time:2.6113624572753906\n",
      "Test:Epoch-30 Loss:11058.559097290039 Test Time:3.5987014770507812\n",
      "Test:Epoch-30 NDCG:0.6784145662064865 RECALL:0.6547766172562255\n",
      "Test:Epoch-30 Jaccard:0.3601687554787065 f1:0.45916013800939975 prauc:0.5279796182912481 roauc:0.8826299538958869\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-31:141643.6568145752 Train Time:2.645557403564453\n",
      "Test:Epoch-31 Loss:11022.433444976807 Test Time:3.637174367904663\n",
      "Test:Epoch-31 NDCG:0.6802129101172394 RECALL:0.6565374085765601\n",
      "Test:Epoch-31 Jaccard:0.36711945839188903 f1:0.4681975037399813 prauc:0.5289627635105706 roauc:0.8827994227990615\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-32:138433.8222503662 Train Time:2.6482794284820557\n",
      "Test:Epoch-32 Loss:10924.34534072876 Test Time:3.6401467323303223\n",
      "Test:Epoch-32 NDCG:0.6798551411359948 RECALL:0.6560162920277111\n",
      "Test:Epoch-32 Jaccard:0.37093918859824243 f1:0.4731771536420802 prauc:0.5310436427842269 roauc:0.8827714231864215\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-33:136071.58688354492 Train Time:2.5998101234436035\n",
      "Test:Epoch-33 Loss:10934.93197631836 Test Time:3.583098888397217\n",
      "Test:Epoch-33 NDCG:0.6829270127721461 RECALL:0.6585765602894474\n",
      "Test:Epoch-33 Jaccard:0.36695555667823204 f1:0.46929424359133604 prauc:0.5298406076681448 roauc:0.8828888371736513\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-34:136829.1388015747 Train Time:2.6654958724975586\n",
      "Test:Epoch-34 Loss:10937.40837097168 Test Time:3.6517550945281982\n",
      "Test:Epoch-34 NDCG:0.685283529861038 RECALL:0.660579460416328\n",
      "Test:Epoch-34 Jaccard:0.3696000369940011 f1:0.4660424643458067 prauc:0.5362214529598994 roauc:0.8833724458435872\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-35:141045.00944519043 Train Time:2.592521905899048\n",
      "Test:Epoch-35 Loss:10904.80111694336 Test Time:3.58463716506958\n",
      "Test:Epoch-35 NDCG:0.6846091498077462 RECALL:0.660321167866029\n",
      "Test:Epoch-35 Jaccard:0.367661842464796 f1:0.46787694138962316 prauc:0.5367181818055757 roauc:0.8835313124040334\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-36:139317.39721679688 Train Time:2.6666691303253174\n",
      "Test:Epoch-36 Loss:10902.314056396484 Test Time:3.640090227127075\n",
      "Test:Epoch-36 NDCG:0.6846238083157691 RECALL:0.6604357487718008\n",
      "Test:Epoch-36 Jaccard:0.3727577456671252 f1:0.47272844360428234 prauc:0.5357379894189951 roauc:0.883352538495985\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-37:139115.72079467773 Train Time:2.5480830669403076\n",
      "Test:Epoch-37 Loss:10931.991676330566 Test Time:3.5292606353759766\n",
      "Test:Epoch-37 NDCG:0.6870532344884418 RECALL:0.6617123224790433\n",
      "Test:Epoch-37 Jaccard:0.36849261199515493 f1:0.46928604082835423 prauc:0.5387527242243059 roauc:0.8838800598706498\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-38:142556.47609710693 Train Time:2.577507257461548\n",
      "Test:Epoch-38 Loss:10875.205604553223 Test Time:3.566772937774658\n",
      "Test:Epoch-38 NDCG:0.6872555773023121 RECALL:0.6623014107516552\n",
      "Test:Epoch-38 Jaccard:0.36687028660395565 f1:0.4669580600412409 prauc:0.5377013999418884 roauc:0.8837855586908376\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-39:135622.5713043213 Train Time:2.685969352722168\n",
      "Test:Epoch-39 Loss:10889.123634338379 Test Time:3.6762404441833496\n",
      "Test:Epoch-39 NDCG:0.6914219782416832 RECALL:0.6661317792231329\n",
      "Test:Epoch-39 Jaccard:0.3860002592560401 f1:0.48622624496763 prauc:0.5413945440590655 roauc:0.884678007978578\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-40:140222.23191833496 Train Time:2.6624722480773926\n",
      "Test:Epoch-40 Loss:10861.404243469238 Test Time:3.6535942554473877\n",
      "Test:Epoch-40 NDCG:0.6901912214356603 RECALL:0.6648681525108928\n",
      "Test:Epoch-40 Jaccard:0.3772211695251172 f1:0.47734339731889025 prauc:0.5406325875092686 roauc:0.8845898333930403\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-41:137674.08032226562 Train Time:2.6784698963165283\n",
      "Test:Epoch-41 Loss:10858.973693847656 Test Time:3.663039445877075\n",
      "Test:Epoch-41 NDCG:0.689767304857723 RECALL:0.664437664927061\n",
      "Test:Epoch-41 Jaccard:0.3760093287377571 f1:0.4790409973700507 prauc:0.5452254506523908 roauc:0.884785990105908\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-42:134584.16650390625 Train Time:2.662787437438965\n",
      "Test:Epoch-42 Loss:10854.922653198242 Test Time:3.6567299365997314\n",
      "Test:Epoch-42 NDCG:0.6906073210508515 RECALL:0.6654016275549715\n",
      "Test:Epoch-42 Jaccard:0.37085613868178735 f1:0.4713751012185332 prauc:0.5499587452337019 roauc:0.8852118878753719\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-43:135698.85346984863 Train Time:2.6505556106567383\n",
      "Test:Epoch-43 Loss:10991.307033538818 Test Time:3.645983934402466\n",
      "Test:Epoch-43 NDCG:0.691985113837503 RECALL:0.6671721291215579\n",
      "Test:Epoch-43 Jaccard:0.37000484810556133 f1:0.4715705973670804 prauc:0.5487944077434973 roauc:0.885203444380072\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-44:132590.62174987793 Train Time:2.6969568729400635\n",
      "Test:Epoch-44 Loss:10951.482025146484 Test Time:3.681270122528076\n",
      "Test:Epoch-44 NDCG:0.6917972862835857 RECALL:0.6667797763230061\n",
      "Test:Epoch-44 Jaccard:0.37033269984506784 f1:0.47056571235710914 prauc:0.5530423568639545 roauc:0.8854785899927327\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-45:136239.0251312256 Train Time:2.7004992961883545\n",
      "Test:Epoch-45 Loss:10896.27615737915 Test Time:3.6864497661590576\n",
      "Test:Epoch-45 NDCG:0.6912503292356531 RECALL:0.6662464189788819\n",
      "Test:Epoch-45 Jaccard:0.3700795779968918 f1:0.4700633950042703 prauc:0.5541797784297318 roauc:0.8853821006569199\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-46:131297.60865020752 Train Time:2.689225435256958\n",
      "Test:Epoch-46 Loss:10972.12329864502 Test Time:3.678057909011841\n",
      "Test:Epoch-46 NDCG:0.6924489673970103 RECALL:0.6680000894519652\n",
      "Test:Epoch-46 Jaccard:0.3809402102230197 f1:0.47864018555994137 prauc:0.5532772754107442 roauc:0.8858028790877195\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-47:136603.78002929688 Train Time:2.6766200065612793\n",
      "Test:Epoch-47 Loss:10827.519298553467 Test Time:3.6617424488067627\n",
      "Test:Epoch-47 NDCG:0.6947680028892999 RECALL:0.6710438691270663\n",
      "Test:Epoch-47 Jaccard:0.38173349563138803 f1:0.48167734110744326 prauc:0.5540017521868662 roauc:0.8860272156002965\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-48:135094.12855529785 Train Time:2.6582491397857666\n",
      "Test:Epoch-48 Loss:10852.645225524902 Test Time:3.6411635875701904\n",
      "Test:Epoch-48 NDCG:0.6947531906719506 RECALL:0.6707065999072521\n",
      "Test:Epoch-48 Jaccard:0.3741561934871263 f1:0.4722648392647073 prauc:0.554652827635082 roauc:0.8858466693332726\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-49:133591.3504486084 Train Time:2.6646018028259277\n",
      "Test:Epoch-49 Loss:10655.728076934814 Test Time:3.654350519180298\n",
      "Test:Epoch-49 NDCG:0.6941002257279073 RECALL:0.6699609118450878\n",
      "Test:Epoch-49 Jaccard:0.37666254333056925 f1:0.47437293314849704 prauc:0.557366376405143 roauc:0.8861231411471434\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-50:132042.31899261475 Train Time:2.6584181785583496\n",
      "Test:Epoch-50 Loss:10695.432998657227 Test Time:3.6472644805908203\n",
      "Test:Epoch-50 NDCG:0.696280476840933 RECALL:0.6723975363045507\n",
      "Test:Epoch-50 Jaccard:0.3846849569096567 f1:0.48111173081825914 prauc:0.5577801681274214 roauc:0.886320849214605\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-51:132494.2276916504 Train Time:2.7002878189086914\n",
      "Test:Epoch-51 Loss:10520.274475097656 Test Time:3.6981136798858643\n",
      "Test:Epoch-51 NDCG:0.6951221956740861 RECALL:0.6709092203790405\n",
      "Test:Epoch-51 Jaccard:0.3822805841973868 f1:0.4816028588051426 prauc:0.5577806611417466 roauc:0.8862689434083328\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-52:131870.8952178955 Train Time:2.7012133598327637\n",
      "Test:Epoch-52 Loss:10525.00410079956 Test Time:3.6882238388061523\n",
      "Test:Epoch-52 NDCG:0.6959540861227793 RECALL:0.6723049064403057\n",
      "Test:Epoch-52 Jaccard:0.38034783324342863 f1:0.47749952150441544 prauc:0.5567016615567437 roauc:0.8865005273461443\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-53:135006.98616027832 Train Time:2.6334757804870605\n",
      "Test:Epoch-53 Loss:10531.920860290527 Test Time:3.622633695602417\n",
      "Test:Epoch-53 NDCG:0.6934610482200602 RECALL:0.6695219498645271\n",
      "Test:Epoch-53 Jaccard:0.3804126859183955 f1:0.47988318098432287 prauc:0.5547579606931073 roauc:0.8859856268699062\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-54:134382.93405914307 Train Time:2.6746253967285156\n",
      "Test:Epoch-54 Loss:10553.171592712402 Test Time:3.657064199447632\n",
      "Test:Epoch-54 NDCG:0.6935203942351229 RECALL:0.6692539470679761\n",
      "Test:Epoch-54 Jaccard:0.3796104430280613 f1:0.4807957087320872 prauc:0.5544513433821618 roauc:0.8859714556039772\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-55:132383.9195098877 Train Time:2.5965383052825928\n",
      "Test:Epoch-55 Loss:10584.681442260742 Test Time:3.5810272693634033\n",
      "Test:Epoch-55 NDCG:0.6919210532872982 RECALL:0.6684596489245751\n",
      "Test:Epoch-55 Jaccard:0.3799015331233928 f1:0.4783031315249912 prauc:0.5542207585386576 roauc:0.8851628804526828\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-56:129002.11241912842 Train Time:2.630544900894165\n",
      "Test:Epoch-56 Loss:10557.436553955078 Test Time:3.6130921840667725\n",
      "Test:Epoch-56 NDCG:0.6906766082539921 RECALL:0.6669001833765289\n",
      "Test:Epoch-56 Jaccard:0.38172449718126716 f1:0.4799235856212153 prauc:0.5537115565641826 roauc:0.8850878211452317\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-57:128569.59586334229 Train Time:2.6087801456451416\n",
      "Test:Epoch-57 Loss:10576.13217163086 Test Time:3.5980350971221924\n",
      "Test:Epoch-57 NDCG:0.6903948901544591 RECALL:0.6663583516356759\n",
      "Test:Epoch-57 Jaccard:0.37571564288530035 f1:0.47517643782397645 prauc:0.5543102882767152 roauc:0.8849495797202316\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-58:176964.04776000977 Train Time:2.677767515182495\n",
      "Test:Epoch-58 Loss:10577.355587005615 Test Time:3.661438226699829\n",
      "Test:Epoch-58 NDCG:0.6904180657050655 RECALL:0.6661473744671131\n",
      "Test:Epoch-58 Jaccard:0.37728898797414295 f1:0.4749631599142203 prauc:0.5518012252707359 roauc:0.8849295690738328\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-59:134491.81072998047 Train Time:2.6533353328704834\n",
      "Test:Epoch-59 Loss:10570.552890777588 Test Time:3.6375222206115723\n",
      "Test:Epoch-59 NDCG:0.6896569342540745 RECALL:0.6653608445207136\n",
      "Test:Epoch-59 Jaccard:0.3791884479486437 f1:0.47623828580550587 prauc:0.5537134589218174 roauc:0.8841191632448401\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-60:131098.185546875 Train Time:2.6796534061431885\n",
      "Test:Epoch-60 Loss:10603.537155151367 Test Time:3.669156789779663\n",
      "Test:Epoch-60 NDCG:0.6924794799323922 RECALL:0.6686934010343469\n",
      "Test:Epoch-60 Jaccard:0.38524428263416843 f1:0.48171347039977697 prauc:0.5542761084376527 roauc:0.8834540290736909\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-61:130883.9224243164 Train Time:2.647909641265869\n",
      "Test:Epoch-61 Loss:10602.528255462646 Test Time:3.645972490310669\n",
      "Test:Epoch-61 NDCG:0.6934985808227789 RECALL:0.6702282084419112\n",
      "Test:Epoch-61 Jaccard:0.38653670509298566 f1:0.48460592334980584 prauc:0.5561683739144215 roauc:0.8835614898066817\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-62:129831.11611175537 Train Time:2.512866497039795\n",
      "Test:Epoch-62 Loss:10599.620010375977 Test Time:3.49308443069458\n",
      "Test:Epoch-62 NDCG:0.6940897916788196 RECALL:0.6712639680421081\n",
      "Test:Epoch-62 Jaccard:0.38331101561117875 f1:0.48106909137520404 prauc:0.5608744396659782 roauc:0.8842852195635929\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-63:126081.11973571777 Train Time:2.6765201091766357\n",
      "Test:Epoch-63 Loss:10595.991645812988 Test Time:3.6408605575561523\n",
      "Test:Epoch-63 NDCG:0.6971205408860676 RECALL:0.6738274730525952\n",
      "Test:Epoch-63 Jaccard:0.38301174989592607 f1:0.47965387058324394 prauc:0.5607695978124125 roauc:0.8842466084850185\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-64:133414.47290802002 Train Time:2.6448535919189453\n",
      "Test:Epoch-64 Loss:10585.105556488037 Test Time:3.6380953788757324\n",
      "Test:Epoch-64 NDCG:0.6964990542140171 RECALL:0.673652688620062\n",
      "Test:Epoch-64 Jaccard:0.38279357492244936 f1:0.4800836104261879 prauc:0.5618608343085931 roauc:0.8840310798345052\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-65:128580.21913909912 Train Time:2.645106315612793\n",
      "Test:Epoch-65 Loss:10565.446166992188 Test Time:3.628192186355591\n",
      "Test:Epoch-65 NDCG:0.6958468269666062 RECALL:0.6733374292917521\n",
      "Test:Epoch-65 Jaccard:0.3861523558913445 f1:0.48292066823877594 prauc:0.5630992456465272 roauc:0.8839972033545759\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-66:126009.1494216919 Train Time:2.5757570266723633\n",
      "Test:Epoch-66 Loss:10579.87680053711 Test Time:3.5607242584228516\n",
      "Test:Epoch-66 NDCG:0.6979777100942984 RECALL:0.674527258132478\n",
      "Test:Epoch-66 Jaccard:0.383455949524465 f1:0.4798696734499441 prauc:0.5613193231100992 roauc:0.8840056709073304\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-67:134544.85906982422 Train Time:2.585531234741211\n",
      "Test:Epoch-67 Loss:10567.072578430176 Test Time:3.57468843460083\n",
      "Test:Epoch-67 NDCG:0.7004189283468778 RECALL:0.678252755944436\n",
      "Test:Epoch-67 Jaccard:0.38599116580356385 f1:0.4827304302282712 prauc:0.5663750930251471 roauc:0.8837720196538521\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-68:131489.45332336426 Train Time:2.650484561920166\n",
      "Test:Epoch-68 Loss:10602.611953735352 Test Time:3.642707109451294\n",
      "Test:Epoch-68 NDCG:0.7001314595108198 RECALL:0.6780048209901389\n",
      "Test:Epoch-68 Jaccard:0.38232298786948055 f1:0.478540029845087 prauc:0.5673337975272159 roauc:0.8840947740604802\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-69:123560.80476379395 Train Time:2.596658945083618\n",
      "Test:Epoch-69 Loss:10647.995868682861 Test Time:3.5832862854003906\n",
      "Test:Epoch-69 NDCG:0.701327008320491 RECALL:0.6786994272720207\n",
      "Test:Epoch-69 Jaccard:0.3811250422588106 f1:0.4766344835580034 prauc:0.5697437220244288 roauc:0.8842910246492393\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-70:130959.1153793335 Train Time:2.6043317317962646\n",
      "Test:Epoch-70 Loss:10597.440628051758 Test Time:3.5919594764709473\n",
      "Test:Epoch-70 NDCG:0.699634272933881 RECALL:0.6779290810693744\n",
      "Test:Epoch-70 Jaccard:0.38577807003581877 f1:0.4815467295075778 prauc:0.567944381147042 roauc:0.8843881567536857\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-71:130723.07897949219 Train Time:2.6870503425598145\n",
      "Test:Epoch-71 Loss:10620.502849578857 Test Time:3.6737983226776123\n",
      "Test:Epoch-71 NDCG:0.7002412609348634 RECALL:0.6780230056331195\n",
      "Test:Epoch-71 Jaccard:0.37932207361244874 f1:0.47587279043233366 prauc:0.5657139195087663 roauc:0.883789869136342\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-72:126229.45352172852 Train Time:2.688572883605957\n",
      "Test:Epoch-72 Loss:10587.555221557617 Test Time:3.6210782527923584\n",
      "Test:Epoch-72 NDCG:0.7029115641268028 RECALL:0.6806331198256155\n",
      "Test:Epoch-72 Jaccard:0.38543267951913956 f1:0.4816868920457827 prauc:0.5680281538897421 roauc:0.8846064756808315\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-73:130909.10069274902 Train Time:2.6626322269439697\n",
      "Test:Epoch-73 Loss:10618.013397216797 Test Time:3.650646448135376\n",
      "Test:Epoch-73 NDCG:0.7018225981558731 RECALL:0.6790981947180965\n",
      "Test:Epoch-73 Jaccard:0.3859997986425393 f1:0.4835219120940261 prauc:0.5695934848621048 roauc:0.8839352143532696\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-74:127311.03887939453 Train Time:2.6348867416381836\n",
      "Test:Epoch-74 Loss:10682.655727386475 Test Time:3.622162342071533\n",
      "Test:Epoch-74 NDCG:0.7018989151929433 RECALL:0.6799119133540013\n",
      "Test:Epoch-74 Jaccard:0.38292604622131543 f1:0.4785892872760736 prauc:0.5696462700985173 roauc:0.884187393917189\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-75:125322.47413635254 Train Time:2.672633647918701\n",
      "Test:Epoch-75 Loss:10652.232124328613 Test Time:3.6573140621185303\n",
      "Test:Epoch-75 NDCG:0.7035841437712589 RECALL:0.680989750687956\n",
      "Test:Epoch-75 Jaccard:0.38545959659336493 f1:0.48338497192493923 prauc:0.569098896847932 roauc:0.8843652733615507\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-76:126593.55826568604 Train Time:2.6076245307922363\n",
      "Test:Epoch-76 Loss:10717.151161193848 Test Time:3.594933032989502\n",
      "Test:Epoch-76 NDCG:0.7002821054679835 RECALL:0.678453434366974\n",
      "Test:Epoch-76 Jaccard:0.37897768354538336 f1:0.476854214993549 prauc:0.5681569618768367 roauc:0.8836703585802426\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-77:127627.34706115723 Train Time:2.6329405307769775\n",
      "Test:Epoch-77 Loss:10678.637580871582 Test Time:3.6189119815826416\n",
      "Test:Epoch-77 NDCG:0.7025639953525908 RECALL:0.6801119444267891\n",
      "Test:Epoch-77 Jaccard:0.38665040324909816 f1:0.485707640193774 prauc:0.5722990321095005 roauc:0.8837873174078055\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-78:133639.45332336426 Train Time:2.647197961807251\n",
      "Test:Epoch-78 Loss:10679.519416809082 Test Time:3.632964849472046\n",
      "Test:Epoch-78 NDCG:0.703374585028141 RECALL:0.6810130552789604\n",
      "Test:Epoch-78 Jaccard:0.3877771449141759 f1:0.48607881152742494 prauc:0.5724206066019454 roauc:0.8838145279554074\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-79:134181.1575164795 Train Time:2.626542329788208\n",
      "Test:Epoch-79 Loss:10678.33218383789 Test Time:3.6154909133911133\n",
      "Test:Epoch-79 NDCG:0.7027325329560472 RECALL:0.6798193423397335\n",
      "Test:Epoch-79 Jaccard:0.38482903719445316 f1:0.4847152538257039 prauc:0.569005406214175 roauc:0.8837664644298734\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-80:126994.39775848389 Train Time:2.6299257278442383\n",
      "Test:Epoch-80 Loss:10730.58532333374 Test Time:3.6150903701782227\n",
      "Test:Epoch-80 NDCG:0.7032246528594535 RECALL:0.680831797348926\n",
      "Test:Epoch-80 Jaccard:0.37805536859370464 f1:0.4765101290290811 prauc:0.5693194428046248 roauc:0.884369582201862\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-81:128237.21047973633 Train Time:2.6299431324005127\n",
      "Test:Epoch-81 Loss:10727.174133300781 Test Time:3.6173551082611084\n",
      "Test:Epoch-81 NDCG:0.7058011657941259 RECALL:0.6822158311146889\n",
      "Test:Epoch-81 Jaccard:0.37910224631268674 f1:0.47816314500413903 prauc:0.5671341259307944 roauc:0.884106203186133\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-82:123836.72931671143 Train Time:2.6484291553497314\n",
      "Test:Epoch-82 Loss:10714.765342712402 Test Time:3.6310298442840576\n",
      "Test:Epoch-82 NDCG:0.7073008261591509 RECALL:0.6828366395250568\n",
      "Test:Epoch-82 Jaccard:0.3850695525246912 f1:0.48386110099477336 prauc:0.5726294058164582 roauc:0.8844040488849828\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-83:127365.4903717041 Train Time:2.6660521030426025\n",
      "Test:Epoch-83 Loss:10760.77458190918 Test Time:3.6549103260040283\n",
      "Test:Epoch-83 NDCG:0.7084774823196163 RECALL:0.6841106238332989\n",
      "Test:Epoch-83 Jaccard:0.3828650685910066 f1:0.48068803857115916 prauc:0.5766031028562238 roauc:0.8839898517612113\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-84:124013.5650177002 Train Time:2.6237266063690186\n",
      "Test:Epoch-84 Loss:10766.976459503174 Test Time:3.6107351779937744\n",
      "Test:Epoch-84 NDCG:0.7107622201580397 RECALL:0.6857594236468623\n",
      "Test:Epoch-84 Jaccard:0.37963206813125244 f1:0.4767710647688577 prauc:0.575183398029921 roauc:0.8843289335227141\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-85:123525.94770050049 Train Time:2.6167454719543457\n",
      "Test:Epoch-85 Loss:10793.787242889404 Test Time:3.5941567420959473\n",
      "Test:Epoch-85 NDCG:0.7065806226673983 RECALL:0.6824695922167371\n",
      "Test:Epoch-85 Jaccard:0.3747089325392751 f1:0.4722754343184245 prauc:0.571745987099475 roauc:0.8836739239275612\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-86:123502.62936401367 Train Time:2.642773151397705\n",
      "Test:Epoch-86 Loss:10786.001159667969 Test Time:3.6298413276672363\n",
      "Test:Epoch-86 NDCG:0.7057928674959617 RECALL:0.6819801958056442\n",
      "Test:Epoch-86 Jaccard:0.3743242936146688 f1:0.47169188460091444 prauc:0.5733861155443802 roauc:0.8838249266587856\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-87:124674.1146697998 Train Time:2.6837081909179688\n",
      "Test:Epoch-87 Loss:10755.799453735352 Test Time:3.669494152069092\n",
      "Test:Epoch-87 NDCG:0.7074400127350624 RECALL:0.6843695637333482\n",
      "Test:Epoch-87 Jaccard:0.3788959182923294 f1:0.4760108202580134 prauc:0.5751895171115418 roauc:0.8841732410356588\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-88:129119.69696044922 Train Time:2.668088912963867\n",
      "Test:Epoch-88 Loss:10794.446254730225 Test Time:3.6558892726898193\n",
      "Test:Epoch-88 NDCG:0.7074041693583173 RECALL:0.6843637375855971\n",
      "Test:Epoch-88 Jaccard:0.3714855427335036 f1:0.4680339444325626 prauc:0.5726524350009785 roauc:0.8836997642667302\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-89:128870.35763549805 Train Time:2.6560378074645996\n",
      "Test:Epoch-89 Loss:10834.811248779297 Test Time:3.6444904804229736\n",
      "Test:Epoch-89 NDCG:0.7099528668672483 RECALL:0.6863349175747215\n",
      "Test:Epoch-89 Jaccard:0.37192709411306474 f1:0.4686362332077714 prauc:0.5718802809674582 roauc:0.8836051403977896\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-90:128524.57427978516 Train Time:2.634246349334717\n",
      "Test:Epoch-90 Loss:10810.786907196045 Test Time:3.6371638774871826\n",
      "Test:Epoch-90 NDCG:0.7090892835903747 RECALL:0.6856908045733492\n",
      "Test:Epoch-90 Jaccard:0.37827960511484165 f1:0.47434587078575174 prauc:0.5783705855940513 roauc:0.8843820022845953\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-91:122773.5265045166 Train Time:2.6469011306762695\n",
      "Test:Epoch-91 Loss:10836.359325408936 Test Time:3.6377365589141846\n",
      "Test:Epoch-91 NDCG:0.7087948973575712 RECALL:0.6852311862507617\n",
      "Test:Epoch-91 Jaccard:0.3778658897745358 f1:0.4736546301367336 prauc:0.5730058273081781 roauc:0.8841108856426407\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-92:129603.6332397461 Train Time:2.7030704021453857\n",
      "Test:Epoch-92 Loss:10859.620780944824 Test Time:3.689958333969116\n",
      "Test:Epoch-92 NDCG:0.7110937808998122 RECALL:0.6874677796374369\n",
      "Test:Epoch-92 Jaccard:0.3741322887733981 f1:0.46976505086571296 prauc:0.5740337541832785 roauc:0.8841261642466647\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-93:121178.23292541504 Train Time:2.6731181144714355\n",
      "Test:Epoch-93 Loss:10836.825630187988 Test Time:3.667185068130493\n",
      "Test:Epoch-93 NDCG:0.7128403421942184 RECALL:0.6896040338128426\n",
      "Test:Epoch-93 Jaccard:0.37537651310081976 f1:0.4718182160630102 prauc:0.5799107860169503 roauc:0.884486551185173\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-94:122790.22763824463 Train Time:2.60148286819458\n",
      "Test:Epoch-94 Loss:10870.232822418213 Test Time:3.573293924331665\n",
      "Test:Epoch-94 NDCG:0.7133082303986243 RECALL:0.6902449100654643\n",
      "Test:Epoch-94 Jaccard:0.3814883774916401 f1:0.47762782723678987 prauc:0.5767562538455631 roauc:0.8844462856071709\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-95:125525.19303894043 Train Time:2.6601831912994385\n",
      "Test:Epoch-95 Loss:10887.152854919434 Test Time:3.6475508213043213\n",
      "Test:Epoch-95 NDCG:0.7138389926243062 RECALL:0.6905685849405259\n",
      "Test:Epoch-95 Jaccard:0.36946944663094744 f1:0.46511415899670383 prauc:0.5722770175926764 roauc:0.8840515200818413\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-96:125424.95100402832 Train Time:2.6508169174194336\n",
      "Test:Epoch-96 Loss:10898.23892211914 Test Time:3.6322100162506104\n",
      "Test:Epoch-96 NDCG:0.7132303058503676 RECALL:0.6899568394266598\n",
      "Test:Epoch-96 Jaccard:0.3719762202633328 f1:0.4683173451525817 prauc:0.5750415465745092 roauc:0.8840961831309265\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-97:139250.95023345947 Train Time:2.628096580505371\n",
      "Test:Epoch-97 Loss:10926.056819915771 Test Time:3.6157643795013428\n",
      "Test:Epoch-97 NDCG:0.7150215832494012 RECALL:0.6917402879882486\n",
      "Test:Epoch-97 Jaccard:0.370964294903936 f1:0.46620265856183707 prauc:0.5726124154785581 roauc:0.8840996463731571\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-98:125632.68479919434 Train Time:2.6201300621032715\n",
      "Test:Epoch-98 Loss:10897.848846435547 Test Time:3.6036832332611084\n",
      "Test:Epoch-98 NDCG:0.7141944887055661 RECALL:0.6911673834593897\n",
      "Test:Epoch-98 Jaccard:0.3760951890234109 f1:0.4714011297640508 prauc:0.5751090155327734 roauc:0.8844967483505715\n",
      "Save Best Model!!!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongwook95.lee/study/dignosis_analysis/retain/retain_torch/Retain_torch.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.transpose(F.softmax(torch.transpose(preAlpha, 0, 1)), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:Epoch-99:120793.49655151367 Train Time:2.64599609375\n",
      "Test:Epoch-99 Loss:10893.363807678223 Test Time:3.6314592361450195\n",
      "Test:Epoch-99 NDCG:0.7143901681217434 RECALL:0.6913343996949214\n",
      "Test:Epoch-99 Jaccard:0.3754753222126795 f1:0.4701535792146098 prauc:0.5749944268850034 roauc:0.8846315455353648\n",
      "Save Best Model!!!\n",
      "\n",
      "Best model: Jaccard:0.3754753222126795 f1:0.4701535792146098 prauc:0.5749944268850034 roauc:0.8846315455353648\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    starttime = time.time()\n",
    "    \n",
    "    # Train Process\n",
    "    model.train()\n",
    "    all_loss = 0.0\n",
    "    for batch_index in range(train_batch_num):\n",
    "        patients_batch = patients_train[batch_index * batch_size:(batch_index + 1) * batch_size]\n",
    "        patients_batch_reshape, patients_lengths = model.padTrainMatrix(patients_batch)  # maxlen × n_samples × inputDimSize\n",
    "        batch_x = patients_batch_reshape[0:-1]  # Get the first n-1 as x, to predict the value of the next n-1 days\n",
    "        batch_y = patients_batch_reshape[1:, :, :diagnosis_size]\n",
    "        optimizer.zero_grad()\n",
    "        # h0 = model.initHidden(batch_x.shape[1])\n",
    "        batch_x = torch.tensor(batch_x, device=torch.device('cuda:0'))\n",
    "        batch_y = torch.tensor(batch_y, device=torch.device('cuda:0'))\n",
    "        y_hat = model(batch_x)\n",
    "        mask = out_mask2(y_hat, patients_lengths)\n",
    "        # By mask, set the network output outside the corresponding sequence length to 0\n",
    "        y_hat = y_hat.mul(mask)\n",
    "        batch_y = batch_y.mul(mask)\n",
    "        # (seq_len, batch_size, out_dim)->(seq_len*batch_size*out_dim, 1)->(seq_len*batch_size*out_dim, )\n",
    "        y_hat = y_hat.view(-1, 1).squeeze()\n",
    "        batch_y = batch_y.view(-1, 1).squeeze()\n",
    "\n",
    "        loss = loss_mce(y_hat, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss += loss.item()\n",
    "    print(\"Train:Epoch-\" + str(epoch) + \":\" + str(all_loss) + \" Train Time:\" + str(time.time() - starttime))\n",
    "\n",
    "    # Evaluation Process\n",
    "    model.eval()\n",
    "    NDCG = 0.0\n",
    "    RECALL = 0.0\n",
    "    DAYNUM = 0.0\n",
    "    all_loss = 0.0\n",
    "    gbert_pred = []\n",
    "    gbert_true = []\n",
    "    gbert_len = []\n",
    "\n",
    "    for batch_index in range(test_batch_num):\n",
    "        patients_batch = patients_test[batch_index * batch_size:(batch_index + 1) * batch_size]\n",
    "        patients_batch_reshape, patients_lengths = model.padTrainMatrix(patients_batch)\n",
    "        batch_x = patients_batch_reshape[0:-1]\n",
    "        batch_y = patients_batch_reshape[1:, :, :diagnosis_size]\n",
    "        batch_x = torch.tensor(batch_x, device=torch.device('cuda:0'))\n",
    "        batch_y = torch.tensor(batch_y, device=torch.device('cuda:0'))\n",
    "        y_hat = model(batch_x)\n",
    "        mask = out_mask2(y_hat, patients_lengths)\n",
    "        loss = loss_mce(y_hat.mul(mask), batch_y.mul(mask))\n",
    "\n",
    "        all_loss += loss.item()\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        ndcg, recall, daynum = validation(y_hat, patients_batch, patients_lengths, topk)\n",
    "        NDCG += ndcg\n",
    "        RECALL += recall\n",
    "        DAYNUM += daynum\n",
    "        gbert_pred.append(y_hat)\n",
    "        gbert_true.append(batch_y.cpu())\n",
    "        gbert_len.append(patients_lengths)\n",
    "\n",
    "    avg_NDCG = NDCG / DAYNUM\n",
    "    avg_RECALL = RECALL / DAYNUM\n",
    "    y_pred_all, y_true_all = batch_squeeze(gbert_pred, gbert_true, gbert_len)\n",
    "    acc_container = metric_report(y_pred_all, y_true_all, 0.2)\n",
    "    print(\"Test:Epoch-\" + str(epoch) + \" Loss:\" + str(all_loss) + \" Test Time:\" + str(time.time() - starttime))\n",
    "    print(\"Test:Epoch-\" + str(epoch) + \" NDCG:\" + str(avg_NDCG) + \" RECALL:\" + str(avg_RECALL))\n",
    "    print(\"Test:Epoch-\" + str(epoch) + \" Jaccard:\" + str(acc_container['jaccard']) +\n",
    "            \" f1:\" + str(acc_container['f1']) + \" prauc:\" + str(acc_container['prauc']) + \" roauc:\" + str(\n",
    "            acc_container['auc']))\n",
    "    \n",
    "    if best_metric_value < acc_container[best_metric]:\n",
    "        best_acc_container = acc_container\n",
    "        torch.save(model, save_best_model + '/best_model.pt')\n",
    "        print(\"Save Best Model!!!\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Best model: \" + \"Jaccard:\" + str(best_acc_container['jaccard']) +\n",
    "            \" f1:\" + str(best_acc_container['f1']) + \" prauc:\" + str(best_acc_container['prauc']) + \" roauc:\" + str(\n",
    "            best_acc_container['auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a71ccbc68c1f2e468ff6892dfd73218f70dc17f86fe87d0654af868e89d8853"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch100')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
